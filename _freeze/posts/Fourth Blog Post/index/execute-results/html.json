{
  "hash": "d38e40891aef53e47e1277c0a5bf52cf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-11-25\"\ncategories: [data, code, analysis]\nimage: \"anomaly.jpg\"\n---\n\n\n\n\nAnomaly detection is a process in data analysis and machine learning that involves identifying patterns, events, or data points that deviate significantly from the expected or normal behavior within a dataset. The goal is to identify anomalies or outliers that may indicate errors, fraud, security threats, or other unexpected events. Anomaly detection is used across various domains, including cybersecurity, finance, healthcare, industrial monitoring, and more.\n\n**What is an anomaly?**\n\nAn anomaly, in the context of data analysis or system behavior, refers to an observation or event that deviates significantly from what is considered normal, expected, or typical. Anomalies are also often referred to as outliers, novelties, or deviations. Detecting anomalies is crucial in various fields, including data analysis, cybersecurity, finance, and industrial systems, as anomalies may indicate errors, fraud, security threats, or other unexpected events.\n\nThere are two main types of anomalies:\n\n1.  **Point Anomalies:** These anomalies involve individual data points that are considered unusual when compared to the rest of the data. For example, a sudden spike or drop in temperature, an unusually high transaction amount in financial data, or a single malfunctioning sensor reading in an industrial system.\n\n2.  **Contextual Anomalies:** These anomalies are identified by considering the context or relationships within the data. A data point may not be anomalous on its own, but it becomes an anomaly when taking into account its context. For instance, a sudden increase in website traffic during a holiday season might not be anomalous, but the same increase during an ordinary weekday could be considered unusual.\n\nCommon reasons for anomalies are:\n\n-   data preprocessing errors;\n\n-   noise;\n\n-   fraud;\n\n-   attacks.\n\nAnomalies can be detected through various methods, including statistical techniques, machine learning algorithms, and rule-based systems. Some common approaches include:\n\n-   **Statistical Methods:** Using statistical measures such as mean, standard deviation, or quantiles to identify data points that fall outside a certain threshold.\n\n-   **Machine Learning Algorithms:** Employing supervised or unsupervised machine learning techniques, such as clustering, classification, or autoencoders, to learn patterns in the data and identify anomalies.\n\n-   **Rule-Based Systems:** Defining explicit rules or thresholds based on domain knowledge to flag instances that deviate from the expected behavior.\n\nAnomaly detection is crucial for maintaining the integrity and security of systems, as anomalies may indicate issues that require investigation or intervention. It's an important aspect of data analysis, monitoring, and maintenance in various industries.\n\n## How does anomaly detection work?\n\nThere are several ways of training machine learning algorithms to detect anomalies. Supervised machine learning techniques are used when you have a labeled data set indicating normal vs. abnormal conditions. For example, a bank or credit card company can develop a process for labeling fraudulent credit card transactions after those transactions have been reported. Medical researchers might similarly label images or data sets indicative of future disease diagnosis. In such instances, supervised machine learning models can be trained to detect these known anomalies.\n\nResearchers might start with some previously discovered outliers but suspect that other anomalies also exist. In the scenario of fraudulent credit card transactions, consumers might fail to report suspicious transactions with innocuous-sounding names and of a small value. A data scientist might use reports that include these types of fraudulent transactions to automatically label other like transactions as fraud, using semi-supervised machine learning techniques.\n\nThe supervised and semi-supervised techniques can only detect known anomalies. However, the vast majority of data is unlabeled. In these cases, data scientists might use unsupervised anomaly detection techniques, which can automatically identify exceptional or rare events.\n\nFor example, a cloud cost estimator might look for unusual upticks in data egress charges or processing costs that could be caused by a poorly written algorithm. Similarly, an intrusion detection algorithm might look for novel network traffic patterns or a rise in authentication requests. In both cases, unsupervised machine learning techniques might be used to identify data points indicating things that are well outside the range of normal behavior. In contrast, supervised techniques would have to be explicitly trained using examples of previously known deviant behavior.\n\n## **Anomaly detection techniques**\n\nMany different kinds of machine learning algorithms can be trained to detect anomalies. Some of the most popular anomaly detection methods include the following:\n\n-   Density-based algorithms determine when an outlier differs from a larger, hence denser normal data set, using algorithms like K-nearest neighbor and Isolation Forest.\n\n-   Cluster-based algorithms evaluate how any point differs from clusters of related data using techniques like K-means cluster analysis.\n\n-   Bayesian-network algorithms develop models for estimating the probability that events will occur based on related data and then identifying significant deviations from these predictions.\n\n-   Neural network algorithms train a neural network to predict an expected time series and then flag deviations.\n\n## Implementation\n\nLet's implement anomaly detection in python using a dataset which contains sales data. First, we need to import the necessary libraries and view the data\n\n::: {#55f552eb .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=610 height=429}\n:::\n:::\n\n\n::: {#19599324 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Item_MRP</th>\n      <th>Item_Outlet_Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.927507</td>\n      <td>0.283587</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.072068</td>\n      <td>0.031419</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.468288</td>\n      <td>0.158115</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.640093</td>\n      <td>0.053555</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.095805</td>\n      <td>0.073651</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe next bit of code defines a Python dictionary named *classifiers* that contains instances of various outlier detection algorithms from the scikit-learn library. Each key-value pair in the dictionary represents a different outlier detection algorithm. Hence, the following algorithms were employed for outlier detection:\n\n-   Angle based Outlier Detector (ABOD)\n\n-   Cluster based Local Outlier Factor (CBLOF)\n\n-   Feature Bagging\n\n-   Histogram-based Outlier Detection (HBOS)\n\n-   Isolation Forest\n\n-   K Nearest Neighbors (KNN)\n\n-   Average KNN\\\n\n::: {#60b2f553 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Angle-based Outlier Detector (ABOD)\nclf_abod = ABOD(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_abod.fit(X_train)\ny_train_pred_abod = clf_abod.labels_\ny_test_pred_abod = clf_abod.predict(X_test)\nprint(\"------ Angle-based Outlier Detector (ABOD) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_abod == 0) / len(y_train_pred_abod):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_abod == 0) / len(y_test_pred_abod):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_abod, np.zeros_like(y_test_pred_abod), zero_division=1))\ninliers_train_abod = X_train[y_train_pred_abod == 0]\noutliers_train_abod = X_train[y_train_pred_abod == 1]\ninliers_test_abod = X_test[y_test_pred_abod == 0]\noutliers_test_abod = X_test[y_test_pred_abod == 1]\nplt.scatter(inliers_train_abod[:, 0], inliers_train_abod[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_abod[:, 0], outliers_train_abod[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_abod[:, 0], inliers_test_abod[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_abod[:, 0], outliers_test_abod[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Angle-based Outlier Detector (ABOD)\")\nplt.legend()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#e8d699f1 .cell execution_count=4}\n``` {.python .cell-code}\n# Cluster-based Local Outlier Factor (CBLOF)\nclf_cblof = CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_cblof.fit(X_train)\ny_train_pred_cblof = clf_cblof.labels_\ny_test_pred_cblof = clf_cblof.predict(X_test)\nprint(\"------ Cluster-based Local Outlier Factor (CBLOF) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_cblof == 0) / len(y_train_pred_cblof):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_cblof == 0) / len(y_test_pred_cblof):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_cblof, np.zeros_like(y_test_pred_cblof), zero_division=1))\ninliers_train_cblof = X_train[y_train_pred_cblof == 0]\noutliers_train_cblof = X_train[y_train_pred_cblof == 1]\ninliers_test_cblof = X_test[y_test_pred_cblof == 0]\noutliers_test_cblof = X_test[y_test_pred_cblof == 1]\nplt.scatter(inliers_train_cblof[:, 0], inliers_train_cblof[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_cblof[:, 0], outliers_train_cblof[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_cblof[:, 0], inliers_test_cblof[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_cblof[:, 0], outliers_test_cblof[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Cluster-based Local Outlier Factor (CBLOF)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#eeb73d2f .cell execution_count=5}\n``` {.python .cell-code}\n# Feature Bagging\nclf_feature_bagging = FeatureBagging(KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_feature_bagging.fit(X_train)\ny_train_pred_feature_bagging = clf_feature_bagging.labels_\ny_test_pred_feature_bagging = clf_feature_bagging.predict(X_test)\nprint(\"------ Feature Bagging ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_feature_bagging == 0) / len(y_train_pred_feature_bagging):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_feature_bagging == 0) / len(y_test_pred_feature_bagging):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_feature_bagging, np.zeros_like(y_test_pred_feature_bagging), zero_division=1))\ninliers_train_feature_bagging = X_train[y_train_pred_feature_bagging == 0]\noutliers_train_feature_bagging = X_train[y_train_pred_feature_bagging == 1]\ninliers_test_feature_bagging = X_test[y_test_pred_feature_bagging == 0]\noutliers_test_feature_bagging = X_test[y_test_pred_feature_bagging == 1]\nplt.scatter(inliers_train_feature_bagging[:, 0], inliers_train_feature_bagging[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_feature_bagging[:, 0], outliers_train_feature_bagging[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_feature_bagging[:, 0], inliers_test_feature_bagging[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_feature_bagging[:, 0], outliers_test_feature_bagging[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Feature Bagging\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#76c29bbd .cell execution_count=6}\n``` {.python .cell-code}\n# Histogram-based Outlier Detection (HBOS)\nclf_hbos = HBOS(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_hbos.fit(X_train)\ny_train_pred_hbos = clf_hbos.labels_\ny_test_pred_hbos = clf_hbos.predict(X_test)\nprint(\"------ Histogram-based Outlier Detection (HBOS) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_hbos == 0) / len(y_train_pred_hbos):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_hbos == 0) / len(y_test_pred_hbos):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_hbos, np.zeros_like(y_test_pred_hbos), zero_division=1))\ninliers_train_hbos = X_train[y_train_pred_hbos == 0]\noutliers_train_hbos = X_train[y_train_pred_hbos == 1]\ninliers_test_hbos = X_test[y_test_pred_hbos == 0]\noutliers_test_hbos = X_test[y_test_pred_hbos == 1]\nplt.scatter(inliers_train_hbos[:, 0], inliers_train_hbos[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_hbos[:, 0], outliers_train_hbos[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_hbos[:, 0], inliers_test_hbos[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_hbos[:, 0], outliers_test_hbos[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Histogram-based Outlier Detection (HBOS)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#446a6c88 .cell execution_count=7}\n``` {.python .cell-code}\n# Isolation Forest\nclf_isolation_forest = IsolationForest(contamination=outliers_fraction, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_isolation_forest.fit(X_train)\ny_train_scores_if = clf_isolation_forest.decision_function(X_train)\ny_test_scores_if = clf_isolation_forest.decision_function(X_test)\nthreshold_if = np.percentile(y_train_scores_if, 100 * outliers_fraction)\ny_train_pred_if = (y_train_scores_if > threshold_if).astype(int)\ny_test_pred_if = (y_test_scores_if > threshold_if).astype(int)\nprint(\"------ Isolation Forest ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_if == 0) / len(y_train_pred_if):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_if == 0) / len(y_test_pred_if):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_if, np.zeros_like(y_test_pred_if), zero_division=1))\ninliers_train_if = X_train[y_train_pred_if == 0]\noutliers_train_if = X_train[y_train_pred_if == 1]\ninliers_test_if = X_test[y_test_pred_if == 0]\noutliers_test_if = X_test[y_test_pred_if == 1]\nplt.scatter(inliers_train_if[:, 0], inliers_train_if[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_if[:, 0], outliers_train_if[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_if[:, 0], inliers_test_if[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_if[:, 0], outliers_test_if[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Isolation Forest\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Isolation Forest ------\nTrain Accuracy: 5.02%\nTest Accuracy: 4.57%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.05      1.00      0.09        78\n           1       1.00      0.00      0.00      1627\n\n    accuracy                           0.05      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.05      0.00      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#0dcb9f6d .cell execution_count=8}\n``` {.python .cell-code}\n# K Nearest Neighbors (KNN)\nclf_knn = KNN(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_knn.fit(X_train)\ny_train_pred_knn = clf_knn.labels_\ny_test_pred_knn = clf_knn.predict(X_test)\nprint(\"------ K Nearest Neighbors (KNN) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_knn == 0) / len(y_train_pred_knn):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_knn == 0) / len(y_test_pred_knn):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_knn, np.zeros_like(y_test_pred_knn), zero_division=1))\ninliers_train_knn = X_train[y_train_pred_knn == 0]\noutliers_train_knn = X_train[y_train_pred_knn == 1]\ninliers_test_knn = X_test[y_test_pred_knn == 0]\noutliers_test_knn = X_test[y_test_pred_knn == 1]\nplt.scatter(inliers_train_knn[:, 0], inliers_train_knn[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_knn[:, 0], outliers_train_knn[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_knn[:, 0], inliers_test_knn[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_knn[:, 0], outliers_test_knn[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"K Nearest Neighbors (KNN)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=643 height=505}\n:::\n:::\n\n\n::: {#5bc75e6c .cell execution_count=9}\n``` {.python .cell-code}\n# Average KNN\nclf_avg_knn = KNN(method='mean', contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_avg_knn.fit(X_train)\ny_train_pred_avg_knn = clf_avg_knn.labels_\ny_test_pred_avg_knn = clf_avg_knn.predict(X_test)\nprint(\"------ Average KNN ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_avg_knn == 0) / len(y_train_pred_avg_knn):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_avg_knn == 0) / len(y_test_pred_avg_knn):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_avg_knn, np.zeros_like(y_test_pred_avg_knn), zero_division=1))\ninliers_train_avg_knn = X_train[y_train_pred_avg_knn == 0]\noutliers_train_avg_knn = X_train[y_train_pred_avg_knn == 1]\ninliers_test_avg_knn = X_test[y_test_pred_avg_knn == 0]\noutliers_test_avg_knn = X_test[y_test_pred_avg_knn == 1]\nplt.scatter(inliers_train_avg_knn[:, 0], inliers_train_avg_knn[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_avg_knn[:, 0], outliers_train_avg_knn[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_avg_knn[:, 0], inliers_test_avg_knn[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_avg_knn[:, 0], outliers_test_avg_knn[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Average KNN\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=643 height=505}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}