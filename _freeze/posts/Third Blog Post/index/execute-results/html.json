{
  "hash": "c5ee704c8d53652934dd390175da8c38",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-11-23\"\ncategories: [data, code, analysis]\nimage: \"cluster.png\"\n---\n\n\n\n\nClustering is a type of unsupervised machine learning technique where the goal is to group similar data points together based on certain characteristics or features. The objective is to identify natural patterns or structures within the data without the need for predefined labels.\n\nIn a clustering algorithm, the algorithm tries to partition the dataset into groups, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. The idea is that data points in the same cluster share some underlying patterns or properties, and these clusters can provide insights into the inherent structure of the data.\n\nThere are various clustering algorithms, each with its own approach to defining what constitutes a \"similar\" data point and how to form clusters. Some popular clustering algorithms include K-means clustering, hierarchical clustering, and DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n\nHere's a brief overview of a couple of commonly used clustering algorithms:\n\n1.  **K-means Clustering:**\n\n    -   Divides the data into a specified number of clusters (k).\n\n    -   Assigns each data point to the cluster whose mean (centroid) is closest to that point.\n\n    -   Iteratively refines the cluster assignments until convergence.\n\n2.  **Hierarchical Clustering:**\n\n    -   Builds a hierarchy of clusters in the form of a tree (dendrogram).\n\n    -   Can be agglomerative (bottom-up) or divisive (top-down).\n\n    -   At each step, the algorithm merges or splits clusters based on a defined similarity measure.\n\n3.  **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n\n    -   Groups together data points that are close to each other and have a sufficient number of nearby neighbors.\n\n    -   Can identify clusters with irregular shapes and can also detect outliers as noise.\n\nOther clustering algorithms are: density-based, distribution-based, centroid-based and hierarchical-based clustering.\n\nClustering is used in various applications, including customer segmentation, image segmentation, anomaly detection, and more. It is particularly valuable when the structure of the data is not well-defined or when there is no labeled training data available for supervised learning.\n\n## Implementation\n\nWe will use the [*make_classification()*]{.underline} function of the scikit learn library to create a test binary classification dataset. The dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how \"well\" the clusters were identified.\n\nIt is worth mentionining that the clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters.\n\nAn example of creating and summarizing the synthetic clustering dataset is listed below.\n\n::: {#e9f756bf .cell execution_count=1}\n``` {.python .cell-code}\n# synthetic classification dataset\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# create scatter plot for samples from each class\nfor class_value in range(2):\n\t# get row indexes for samples with this class\n\trow_ix = where(y == class_value)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=569 height=411}\n:::\n:::\n\n\nRunning the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters).\n\nWe can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings.\n\n## K-means clustering algorithm\n\nK-means clustering is the most commonly used clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm.\n\nThis algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n\nK-means is best used on smaller data sets because it iterates over *all* of the data points. That means it'll take more time to classify data points if there are a large amount of them in the data set.\n\nSince this is how k-means clusters data points, it doesn't scale well.\n\n::: {#08e58feb .cell execution_count=2}\n``` {.python .cell-code}\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Mini-Batch K-Means\n\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n::: {#a35861b1 .cell execution_count=3}\n``` {.python .cell-code}\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=569 height=411}\n:::\n:::\n\n\n## **DBSCAN clustering algorithm**\n\nDBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.\n\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\n\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\n\nDBSCAN uses two parameters to determine how clusters are defined: *minPts* (the minimum number of data points that need to be clustered together for an area to be considered high-density) and *eps* (the distance used to determine if a data point is in the same area as other data points).\n\nChoosing the right initial parameters is critical for this algorithm to work.\n\n::: {#51dea119 .cell execution_count=4}\n``` {.python .cell-code}\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=411}\n:::\n:::\n\n\n## **Agglomerative Hierarchy clustering algorithm**\n\nThis is the most common type of hierarchical clustering algorithm. It's used to group objects in clusters based on how similar they are to each other.\n\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\n\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\n\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n::: {#13a566d5 .cell execution_count=5}\n``` {.python .cell-code}\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### BIRCH\n\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n::: {#d758dc4f .cell execution_count=6}\n``` {.python .cell-code}\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Affinity Propagation\n\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n::: {#4d4ce8c8 .cell execution_count=7}\n``` {.python .cell-code}\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Spectral Clustering\n\nSpectral Clustering is a general class of clustering methods, drawn from [linear algebra](https://machinelearningmastery.com/linear-algebra-machine-learning-7-day-mini-course/).\n\n::: {#e465a6ca .cell execution_count=8}\n``` {.python .cell-code}\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=569 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}